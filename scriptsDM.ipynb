{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log2\n",
    "import pandas as pd\n",
    "from statistics import variance, mean, mode\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manhattan\n",
    "def manhattan_dist(p1: tuple,p2: tuple):\n",
    "    val = 0\n",
    "    for i in range(len(p1)):\n",
    "        val += abs(p1[i]-p2[i])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Euclidean\n",
    "def euclidean_dist(p1: tuple,p2: tuple):\n",
    "    val = 0\n",
    "    for i in range(len(p1)):\n",
    "        val += (p1[i]-p2[i])**2\n",
    "    return math.sqrt(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chebyshev\n",
    "def chebyshev_dist(p1: tuple,p2: tuple):\n",
    "    return max([abs(p1[i]-p2[i]) for i in range(len(p1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min-Max-Norm\n",
    "def min_max_norm(l: list):\n",
    "    ma = max(l)\n",
    "    mi = min(l)\n",
    "    li = [(i - mi)/(ma-mi) for i in l]\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z-Norm\n",
    "def z_norm(l: list):\n",
    "    v = variance(l)\n",
    "    m = mean(l)\n",
    "    li = [(i-m)/math.sqrt(v) for i in l]\n",
    "    return li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Call these functions with the arguments (ground-truth, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy, Information Gain, Gain Ratio, Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entropy\n",
    "def entropy(entropy_row, class_row):\n",
    "    data = pd.concat([entropy_row, class_row], axis=1)\n",
    "    data.set_axis(['Entropy', 'Class'], axis=1, inplace=True)\n",
    "    data = data.groupby([l for l in data.columns]).size().reset_index(name='Count')\n",
    "\n",
    "    dic = {}\n",
    "    for val in entropy_row.unique():\n",
    "        d = data[data.Entropy == val]\n",
    "        l = sum(d[\"Count\"])\n",
    "        dic[val] = -sum([((i/l)*log2(i/l)) for i in d[\"Count\"]])\n",
    "\n",
    "    e = 0  \n",
    "    l = len(class_row)  \n",
    "    for i in class_row.value_counts():\n",
    "        e += (-(i/l)*log2(i/l))\n",
    "    return dic, e\n",
    "\n",
    "#Information Gain\n",
    "def inf_gain(entropy_row, class_row):\n",
    "    ent, whole_set = entropy(entropy_row,class_row)\n",
    "    l = len(entropy_row)\n",
    "    t = entropy_row.value_counts()\n",
    "    \n",
    "    for val in entropy_row.unique():\n",
    "        whole_set -= ((t[val]/l)*ent[val])\n",
    "\n",
    "    return whole_set\n",
    "\n",
    "d = {'Genre': [\"Action\", \"Action\", \"Action\", \"Adventure\",\"Adventure\", \"Adventure\",\"Adventure\", \"Adventure\",\"Adventure\", \"Adventure\"],\n",
    "    'Platform': [\"Xbox\",\"Switch\",\"PS4\",\"Switch\",\"Switch\",\"Switch\",\"Switch\",\"PS4\",\"PS4\",\"PS4\"],\n",
    "    'Buy': [\"No\",\"No\",\"No\",\"Yes\",\"Yes\",\"No\",\"No\",\"Yes\",\"Yes\",\"No\"]}\n",
    "df = pd.DataFrame(data=d)\n",
    " \n",
    "#inf_gain(df.Genre, df.Buy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrinsic(entropy_row, class_row):\n",
    "    t = entropy_row.value_counts()\n",
    "    l = len(entropy_row)\n",
    "    s = 0 \n",
    "    for val in entropy_row.unique():\n",
    "        s -= (t[val]/l)*log2(t[val]/l)\n",
    "    return s\n",
    "\n",
    "def gain_ratio(entropy_row, class_row):\n",
    "    return (inf_gain(entropy_row, class_row)/intrinsic(entropy_row,class_row)) \n",
    "\n",
    "d = {'Genre': [\"Action\", \"Action\", \"Action\", \"Adventure\",\"Adventure\", \"Adventure\",\"Adventure\", \"Adventure\",\"Adventure\", \"Adventure\"],\n",
    "    'Platform': [\"Xbox\",\"Switch\",\"PS4\",\"Switch\",\"Switch\",\"Switch\",\"Switch\",\"PS4\",\"PS4\",\"PS4\"],\n",
    "    'Buy': [\"No\",\"No\",\"No\",\"Yes\",\"Yes\",\"No\",\"No\",\"Yes\",\"Yes\",\"No\"]}\n",
    "df = pd.DataFrame(data=d)\n",
    " \n",
    "#gain_ratio(df.Platform, df.Buy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(entropy_row, class_row):\n",
    "    data = pd.concat([entropy_row, class_row], axis=1)\n",
    "    data.set_axis(['Entropy', 'Class'], axis=1, inplace=True)\n",
    "    data = data.groupby([l for l in data.columns]).size().reset_index(name='Count')\n",
    "\n",
    "    dict = {}\n",
    "    for val in entropy_row.unique():\n",
    "        s = 1\n",
    "        d = data[data.Entropy == val]\n",
    "        l = sum(d[\"Count\"])\n",
    "        for i in d[\"Count\"]:\n",
    "            s -= (i/l)**2\n",
    "        dict[val] = s\n",
    "    return dict\n",
    "    \n",
    "def gini_index(entropy_row, class_row):\n",
    "    data_with_gini = gini(entropy_row, class_row)\n",
    "    t = entropy_row.value_counts()\n",
    "    l = len(entropy_row)\n",
    "    s=0\n",
    "    for val in entropy_row.unique():\n",
    "        s += (t[val]/l)*data_with_gini[val]\n",
    "    return s\n",
    "\n",
    "\n",
    "# d = {'Genre': [\"Action\", \"Action\", \"Action\", \"Adventure\",\"Adventure\", \"Adventure\",\"Adventure\", \"Adventure\",\"Adventure\", \"Adventure\"],\n",
    "#     'Platform': [\"Xbox\",\"Switch\",\"PS4\",\"Switch\",\"Switch\",\"Switch\",\"Switch\",\"PS4\",\"PS4\",\"PS4\"],\n",
    "#     'Buy': [\"No\",\"No\",\"No\",\"Yes\",\"Yes\",\"No\",\"No\",\"Yes\",\"Yes\",\"No\"]}\n",
    "# df = pd.DataFrame(data=d)\n",
    " \n",
    "# gini_index(df.Platform, df.Buy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(data, dist_type):\n",
    "    data_ex_class = [p[:-1] for p in data]\n",
    "    mat = [[] for i in range(len(data))]\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range((i),len(data)):\n",
    "            if i == j:\n",
    "                #Not \"Correct\" with inf, but we want to find minimum in the next step, excluding distance to self\n",
    "                mat[i].append(math.inf)\n",
    "            else:\n",
    "                d = dist_type(data_ex_class[i],data_ex_class[j])\n",
    "                mat[i].append(d)\n",
    "                mat[j].append(d)\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def find_k_min(d_matr, k):\n",
    "    mat = d_matr.copy()\n",
    "    l = []\n",
    "    for i in range(len(mat)):\n",
    "        x = np.array(mat[i]).argsort()[:k]\n",
    "        l.append(x)\n",
    "    return l\n",
    "\n",
    "def k_nn(data, k, dist_type): \n",
    "    d_matr = distance_matrix(data, dist_type)\n",
    "    indexes = find_k_min(d_matr, k)\n",
    "    class_at_ind = [[data[i][-1] for i in j] for j in indexes]\n",
    "    pred = [max(set(class_at_ind[i]), key=class_at_ind[i].count) for i in range(len(class_at_ind))]\n",
    "    return pred\n",
    "    \n",
    "\n",
    "#(coord, ..., coord, class)\n",
    "#data = [(1,0.4,0), (1.3,-0.4,0),(1.1,0.8,0),(1,-2,0),(-0.1,0,0),(0,0.3,1),(-0.6,0.9,1),(-1.4,-1.4,1),(-1.3,-0.2,1),(-1,1.5,1)]\n",
    "#k_nn(data, 3, manhattan_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means and SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_cent_point(data, centroids, dist_type):\n",
    "    l = [[] for i in range(len(data))]\n",
    "    for i in range(len(centroids)):\n",
    "        for j in range(len(data)):\n",
    "            l[j].append(dist_type(data[j],centroids[i]))\n",
    "    return l\n",
    "\n",
    "#Returns the new centroid with the points associated\n",
    "def compute_centroids_points(data, dist,median):\n",
    "    d = [[] for i in range(len(dist[0]))]\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        ### THIS SHOULD BE AMENDED TO PURSUE EACH PATH AND FIND THE ONE WITH LOWEST SSE ###\n",
    "        ### Dilemma: Spend time fixing, or just run ~ 100 times and pick solution with lowest SSE? ###\n",
    "        ind = random.choice(np.where(np.array(dist[i]) == np.array(dist[i]).min())[0])\n",
    "\n",
    "        d[ind].append(data[i])\n",
    "    dic = {}\n",
    "    for points in d:\n",
    "        if median:\n",
    "            x = np.median(points, axis=0)\n",
    "        else:\n",
    "            x = np.average(points, axis=0)\n",
    "        dic[tuple(x)] = points \n",
    "    return dic\n",
    "\n",
    "def sse(centroids_with_points: dict, dist_type):\n",
    "    s = 0 \n",
    "    for key,vals in centroids_with_points.items():\n",
    "        for val in vals:\n",
    "            s += dist_type(key,val)**2\n",
    "    return s\n",
    "    \n",
    "#returns result in i iterations\n",
    "def k_means(data, centroids_or_k, dist_type, median):\n",
    "    if type(centroids_or_k) == int:\n",
    "        centroids = random.sample(data,centroids_or_k)\n",
    "    else: \n",
    "        centroids = centroids_or_k\n",
    "    iterations = {}\n",
    "    i = 0\n",
    "    while True:\n",
    "        dist = dist_cent_point(data, centroids, dist_type)\n",
    "        new_c_p = (compute_centroids_points(data,dist,median))\n",
    "        i += 1\n",
    "        new_centroids = [i for i in new_c_p.keys()]\n",
    "        iterations[i] = new_c_p\n",
    "        if set(new_centroids) == set(centroids):\n",
    "            return new_c_p, i, iterations\n",
    "        centroids = [x for x in new_c_p.keys()]\n",
    "        \n",
    "#Because of randomness in picking between ties --> run n times \n",
    "def k_means_runner(data, centroids_or_k, dist_type, n, median, show_it = False):\n",
    "    sol = (None,math.inf,0) #(Solution, SSE, Iterations)\n",
    "    iter = None\n",
    "    #Iterations to run given by num\n",
    "    for i in range(n):\n",
    "        r,i,itr = k_means(data,centroids_or_k,manhattan_dist, median=True)\n",
    "        s = sse(r,manhattan_dist)\n",
    "        if s < sol[1]:\n",
    "            sol = (r,s,i) \n",
    "            iter = itr\n",
    "    if show_it:\n",
    "        return sol, iter\n",
    "    return sol\n",
    "\n",
    "# Median decides how to compute new centroid --> TRUE: Median, FALSE: AV\n",
    "# data = [(2,2),(4,6),(4,8),(6,6),(6,8),(8,0)]\n",
    "# centroids = [(4,6),(4,8),(6,6)]\n",
    "# k_means_runner(data,centroids,manhattan_dist, 50, median=True, show_it = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-based clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical-based clustering (Agglomerative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE = 0\n",
    "COMPLETE = 1\n",
    "AVERAGE = 2\n",
    "\n",
    "def dist_between(c1,c2,linkage,dist_type):\n",
    "    d_list = []\n",
    "\n",
    "    for i in range(len(c1)):\n",
    "        ci = c1[i]\n",
    "        if type(ci) == int or type(ci) == float:\n",
    "            ci = tuple([ci])\n",
    "        for j in range(len(c2)):\n",
    "            cj = c2[j]\n",
    "            if type(cj) == int or type(cj) == float:\n",
    "                cj = tuple([cj])\n",
    "            d_list.append(dist_type(ci,cj))\n",
    "    #\n",
    "    if linkage == 1:\n",
    "        return max(d_list)\n",
    "\n",
    "    elif linkage == 2:\n",
    "        return np.average(d_list)\n",
    "\n",
    "    else:\n",
    "        return min(d_list)\n",
    "\n",
    "\n",
    "def hier_clust(data, dist_type, linkage, k_stop = 1, show_it = False):\n",
    "    clusters = ([[i] for i in data])\n",
    "    iterations = {0: (f\"{len(clusters.copy())} clusters: {clusters.copy()}\")}\n",
    "    c_size = len(clusters)\n",
    "    it = 0\n",
    "    while c_size > k_stop:\n",
    "        min_dist = (None, None, math.inf)\n",
    "        for i in range(c_size):\n",
    "            for j in range (i, c_size):\n",
    "                if i != j:\n",
    "                    d = dist_between(clusters[i],clusters[j],linkage,dist_type)\n",
    "                    if d < min_dist[2]:\n",
    "                        min_dist = (clusters[i],clusters[j],d)\n",
    "        l = min_dist[0] + min_dist[1]\n",
    "        clusters.remove(min_dist[0])\n",
    "        clusters.remove(min_dist[1])\n",
    "        clusters.append(l)\n",
    "        it+=1\n",
    "        iterations[it]=(f\"{len(clusters.copy())} clusters: {clusters.copy()}\")\n",
    "        c_size = len(clusters)\n",
    "    if show_it:\n",
    "        return clusters, iterations\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# linkage: 0 -> single, 1 -> complete, 2 -> average\n",
    "# Can set show_it = True to show iterations\n",
    "# data = [(40),(63),(54),(11),(33),(28)]\n",
    "# hier_clust(data,manhattan_dist,SINGLE, k_stop = 1, show_it = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN BLOCK (RUN ALL FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24cd5c2f30858a0d9bc1d12d892ca26d1a556e72a0d5b4289d39a1c46964c99a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
